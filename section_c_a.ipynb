{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SECTION - C (A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import layers\n",
    "import time\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# Part 1: Analytic mapping between uniform and normal distributions in 2D\n",
    "\n",
    "def uniform_to_normal_analytic(u):\n",
    "    \"\"\"\n",
    "    Map samples from a 2D uniform distribution U(0,1) to a 2D standard normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        u: Array of shape (..., 2) with values in [0, 1]\n",
    "        \n",
    "    Returns:\n",
    "        Array of shape (..., 2) following a standard normal distribution\n",
    "    \"\"\"\n",
    "    # Use the inverse CDF method (Box-Muller transform is an alternative)\n",
    "    z = tfp.math.ndtri(u)  # Inverse of standard normal CDF (probit function)\n",
    "    return z\n",
    "\n",
    "def normal_to_uniform_analytic(z):\n",
    "    \"\"\"\n",
    "    Map samples from a 2D standard normal distribution to a 2D uniform distribution U(0,1).\n",
    "    \n",
    "    Args:\n",
    "        z: Array of shape (..., 2) following a standard normal distribution\n",
    "        \n",
    "    Returns:\n",
    "        Array of shape (..., 2) with values in [0, 1]\n",
    "    \"\"\"\n",
    "    # Apply normal CDF to convert to uniform\n",
    "    u = tfp.distributions.Normal(0., 1.).cdf(z)\n",
    "    return u\n",
    "\n",
    "# Verify the analytic mapping empirically\n",
    "def verify_analytic_mapping():\n",
    "    # Generate uniform samples\n",
    "    n_samples = 10000\n",
    "    u_samples = np.random.uniform(0, 1, size=(n_samples, 2))\n",
    "    \n",
    "    # Map to normal using our analytic function\n",
    "    z_samples = uniform_to_normal_analytic(u_samples)\n",
    "    \n",
    "    # Map back to uniform\n",
    "    u_reconstructed = normal_to_uniform_analytic(z_samples)\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot original uniform distribution\n",
    "    axes[0, 0].scatter(u_samples[:, 0], u_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[0, 0].set_title('Original Uniform Distribution')\n",
    "    axes[0, 0].set_xlim(0, 1)\n",
    "    axes[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot mapped normal distribution\n",
    "    axes[0, 1].scatter(z_samples[:, 0], z_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[0, 1].set_title('Mapped Normal Distribution')\n",
    "    axes[0, 1].set_xlim(-4, 4)\n",
    "    axes[0, 1].set_ylim(-4, 4)\n",
    "    \n",
    "    # Plot reconstructed uniform distribution\n",
    "    axes[1, 0].scatter(u_reconstructed[:, 0], u_reconstructed[:, 1], alpha=0.1, s=1)\n",
    "    axes[1, 0].set_title('Reconstructed Uniform Distribution')\n",
    "    axes[1, 0].set_xlim(0, 1)\n",
    "    axes[1, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot marginal distributions of the normal samples\n",
    "    sns.histplot(z_samples[:, 0], kde=True, stat=\"density\", ax=axes[1, 1])\n",
    "    x = np.linspace(-4, 4, 1000)\n",
    "    axes[1, 1].plot(x, stats.norm.pdf(x, 0, 1), 'r-', lw=2)\n",
    "    axes[1, 1].set_title('Normal Marginal Distribution')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('analytic_mapping_verification.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Compute statistics to verify normality\n",
    "    mean = np.mean(z_samples, axis=0)\n",
    "    std = np.std(z_samples, axis=0)\n",
    "    \n",
    "    print(\"Empirical mean of transformed samples:\", mean)\n",
    "    print(\"Empirical std of transformed samples:\", std)\n",
    "    \n",
    "    # Compute statistics to verify uniformity of reconstructed samples\n",
    "    u_mean = np.mean(u_reconstructed, axis=0)\n",
    "    u_std = np.std(u_reconstructed, axis=0)\n",
    "    \n",
    "    print(\"Empirical mean of reconstructed uniform samples:\", u_mean)\n",
    "    print(\"Empirical std of reconstructed uniform samples:\", u_std)\n",
    "    \n",
    "    # Test uniformity with Kolmogorov-Smirnov test\n",
    "    from scipy.stats import kstest\n",
    "    ks_result1 = kstest(u_reconstructed[:, 0], 'uniform')\n",
    "    ks_result2 = kstest(u_reconstructed[:, 1], 'uniform')\n",
    "    \n",
    "    print(\"KS test for dimension 1:\", ks_result1)\n",
    "    print(\"KS test for dimension 2:\", ks_result2)\n",
    "    \n",
    "    return z_samples, u_reconstructed\n",
    "\n",
    "# Run verification\n",
    "z_samples, u_reconstructed = verify_analytic_mapping()\n",
    "\n",
    "# Part 2: Normalizing flow from uniform to normal distribution\n",
    "\n",
    "class RealNVP(keras.Model):\n",
    "    \"\"\"\n",
    "    Real-valued Non-Volume Preserving (RealNVP) normalizing flow model.\n",
    "    This implements a stack of coupling layers for transforming between\n",
    "    distributions.\n",
    "    \"\"\"\n",
    "    def __init__(self, num_coupling_layers=4, hidden_units=64):\n",
    "        super(RealNVP, self).__init__()\n",
    "        \n",
    "        self.num_coupling_layers = num_coupling_layers\n",
    "        self.hidden_units = hidden_units\n",
    "        \n",
    "        # Build the network\n",
    "        self.s_t_layers = []\n",
    "        \n",
    "        for i in range(num_coupling_layers):\n",
    "            # For alternate masks, we alternate which dimension is transformed\n",
    "            mask = np.array([0, 1]) if i % 2 == 0 else np.array([1, 0])\n",
    "            self.s_t_layers.append(CouplingLayer(mask, hidden_units))\n",
    "    \n",
    "    def call(self, x, inverse=False, training=False):\n",
    "        \"\"\"\n",
    "        Forward pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor\n",
    "            inverse: Whether to run the inverse transformation\n",
    "            training: Whether the model is in training mode\n",
    "            \n",
    "        Returns:\n",
    "            y: Transformed tensor\n",
    "            log_det_jacobian: Log determinant of the Jacobian of the transformation\n",
    "        \"\"\"\n",
    "        if inverse:\n",
    "            # Inverse transformation (normal to uniform)\n",
    "            return self.inverse(x, training=training)\n",
    "        \n",
    "        # Forward transformation (uniform to normal)\n",
    "        log_det_jacobian = 0\n",
    "        y = x\n",
    "        \n",
    "        for layer in self.s_t_layers:\n",
    "            y, ldj = layer(y, training=training)\n",
    "            log_det_jacobian += ldj\n",
    "        \n",
    "        return y, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, y, training=False):\n",
    "        \"\"\"\n",
    "        Inverse pass through the model.\n",
    "        \n",
    "        Args:\n",
    "            y: Input tensor\n",
    "            training: Whether the model is in training mode\n",
    "            \n",
    "        Returns:\n",
    "            x: Transformed tensor\n",
    "            log_det_jacobian: Log determinant of the Jacobian of the transformation\n",
    "        \"\"\"\n",
    "        log_det_jacobian = 0\n",
    "        x = y\n",
    "        \n",
    "        # Apply coupling layers in reverse order\n",
    "        for layer in reversed(self.s_t_layers):\n",
    "            x, ldj = layer(x, inverse=True, training=training)\n",
    "            log_det_jacobian += ldj\n",
    "        \n",
    "        return x, log_det_jacobian\n",
    "\n",
    "class CouplingLayer(keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    Coupling layer as described in the RealNVP paper.\n",
    "    \"\"\"\n",
    "    def __init__(self, mask, hidden_units):\n",
    "        super(CouplingLayer, self).__init__()\n",
    "        \n",
    "        self.mask = tf.constant(mask, dtype=tf.float32)\n",
    "        self.scale_net = self._build_scale_net(mask, hidden_units)\n",
    "        self.translation_net = self._build_translation_net(mask, hidden_units)\n",
    "    \n",
    "    def _build_scale_net(self, mask, hidden_units):\n",
    "        # Network to compute scaling factor s(x)\n",
    "        masked_input = keras.layers.Input(shape=2)\n",
    "        \n",
    "        # Apply mask\n",
    "        x = masked_input * (1 - mask)\n",
    "        \n",
    "        # Hidden layers\n",
    "        x = keras.layers.Dense(hidden_units, activation='relu')(x)\n",
    "        x = keras.layers.Dense(hidden_units, activation='relu')(x)\n",
    "        \n",
    "        # Output layer (scale factor), use tanh for boundedness\n",
    "        x = keras.layers.Dense(sum(mask), activation='tanh')(x)\n",
    "        \n",
    "        # Use tanh to bound the scaling and multiply by a factor to adjust range\n",
    "        scaling = x * 0.5  # This limits the scaling to exp(Â±0.5)\n",
    "        \n",
    "        return keras.Model(inputs=masked_input, outputs=scaling)\n",
    "    \n",
    "    def _build_translation_net(self, mask, hidden_units):\n",
    "        # Network to compute translation t(x)\n",
    "        masked_input = keras.layers.Input(shape=2)\n",
    "        \n",
    "        # Apply mask\n",
    "        x = masked_input * (1 - mask)\n",
    "        \n",
    "        # Hidden layers\n",
    "        x = keras.layers.Dense(hidden_units, activation='relu')(x)\n",
    "        x = keras.layers.Dense(hidden_units, activation='relu')(x)\n",
    "        \n",
    "        # Output layer (translation)\n",
    "        translation = keras.layers.Dense(sum(mask))(x)\n",
    "        \n",
    "        return keras.Model(inputs=masked_input, outputs=translation)\n",
    "    \n",
    "    def call(self, x, inverse=False, training=False):\n",
    "        if inverse:\n",
    "            return self.inverse(x, training=training)\n",
    "        \n",
    "        # Split input based on mask\n",
    "        x_masked = x * self.mask\n",
    "        x_pass = x * (1 - self.mask)\n",
    "        \n",
    "        # Calculate scaling and translation factors\n",
    "        s = self.scale_net(x, training=training)\n",
    "        t = self.translation_net(x, training=training)\n",
    "        \n",
    "        # Apply scale and translate to the masked part\n",
    "        exp_s = tf.exp(s)\n",
    "        transformed_x_masked = x_masked * exp_s + t\n",
    "        \n",
    "        # Combine with the unchanged part\n",
    "        y = x_pass + transformed_x_masked\n",
    "        \n",
    "        # Log determinant of Jacobian\n",
    "        log_det_jacobian = tf.reduce_sum(s, axis=1)\n",
    "        \n",
    "        return y, log_det_jacobian\n",
    "    \n",
    "    def inverse(self, y, training=False):\n",
    "        # Split input based on mask\n",
    "        y_masked = y * self.mask\n",
    "        y_pass = y * (1 - self.mask)\n",
    "        \n",
    "        # Calculate scaling and translation factors using the unchanged part\n",
    "        s = self.scale_net(y_pass, training=training)\n",
    "        t = self.translation_net(y_pass, training=training)\n",
    "        \n",
    "        # Apply inverse transformation to the masked part\n",
    "        exp_s = tf.exp(s)\n",
    "        transformed_y_masked = (y_masked - t) / exp_s\n",
    "        \n",
    "        # Combine with the unchanged part\n",
    "        x = y_pass + transformed_y_masked\n",
    "        \n",
    "        # Log determinant of Jacobian (negative of forward)\n",
    "        log_det_jacobian = -tf.reduce_sum(s, axis=1)\n",
    "        \n",
    "        return x, log_det_jacobian\n",
    "\n",
    "# Loss function for uniform to normal flow\n",
    "def uniform_to_normal_loss(model, x_batch):\n",
    "    \"\"\"\n",
    "    Compute loss for transforming uniform to normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        model: Normalizing flow model\n",
    "        x_batch: Batch of uniform samples\n",
    "        \n",
    "    Returns:\n",
    "        Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    # Forward pass through the flow\n",
    "    z, ldj = model(x_batch, inverse=False)\n",
    "    \n",
    "    # Compute log probability of z under standard normal\n",
    "    log_prob_z = tf.reduce_sum(\n",
    "        -0.5 * z**2 - 0.5 * tf.math.log(2 * np.pi), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Compute log probability of x under the flow\n",
    "    log_prob_x = log_prob_z + ldj\n",
    "    \n",
    "    # Return negative log likelihood\n",
    "    return -tf.reduce_mean(log_prob_x)\n",
    "\n",
    "# Loss function for normal to uniform flow\n",
    "def normal_to_uniform_loss(model, z_batch):\n",
    "    \"\"\"\n",
    "    Compute loss for transforming normal to uniform distribution.\n",
    "    \n",
    "    Args:\n",
    "        model: Normalizing flow model\n",
    "        z_batch: Batch of normal samples\n",
    "        \n",
    "    Returns:\n",
    "        Negative log likelihood loss\n",
    "    \"\"\"\n",
    "    # Forward pass through the flow (normal to uniform)\n",
    "    u, ldj = model(z_batch, inverse=False)\n",
    "    \n",
    "    # Compute log probability of u under uniform distribution\n",
    "    # For U(0,1), the log PDF is 0 inside the domain and -inf outside\n",
    "    # We'll use a soft penalty for being outside [0,1]\n",
    "    in_range = tf.logical_and(\n",
    "        tf.logical_and(u[:, 0] >= 0, u[:, 0] <= 1),\n",
    "        tf.logical_and(u[:, 1] >= 0, u[:, 1] <= 1)\n",
    "    )\n",
    "    in_range = tf.cast(in_range, tf.float32)\n",
    "    \n",
    "    # Soft constraint to keep values in [0,1]\n",
    "    penalty = 100.0 * tf.reduce_sum(\n",
    "        tf.maximum(0.0, -u) + tf.maximum(0.0, u - 1.0), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # The log probability under uniform is 0 for in-range values\n",
    "    log_prob_u = -penalty\n",
    "    \n",
    "    # Compute log probability of z under the flow\n",
    "    log_prob_z = log_prob_u + ldj\n",
    "    \n",
    "    # Add log probability of z under standard normal\n",
    "    prior_log_prob = tf.reduce_sum(\n",
    "        -0.5 * z_batch**2 - 0.5 * tf.math.log(2 * np.pi), \n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    # Return negative log likelihood\n",
    "    return -tf.reduce_mean(log_prob_z + prior_log_prob)\n",
    "\n",
    "# Train the uniform to normal flow\n",
    "def train_uniform_to_normal_flow(n_samples=10000, epochs=100, batch_size=256):\n",
    "    # Create a dataset of uniform samples\n",
    "    uniform_samples = np.random.uniform(0, 1, size=(n_samples, 2))\n",
    "    \n",
    "    # Create the flow model\n",
    "    flow_model = RealNVP(num_coupling_layers=8, hidden_units=128)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(x_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = uniform_to_normal_loss(flow_model, x_batch)\n",
    "        \n",
    "        gradients = tape.gradient(loss, flow_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, flow_model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(uniform_samples)\n",
    "    dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for x_batch in dataset:\n",
    "            loss = train_step(x_batch)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        epoch_loss /= n_batches\n",
    "        train_losses.append(epoch_loss.numpy())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss (Uniform to Normal)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('u2n_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_samples = np.random.uniform(0, 1, size=(5000, 2))\n",
    "    transformed_samples, _ = flow_model(test_samples)\n",
    "    transformed_samples = transformed_samples.numpy()\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].scatter(test_samples[:, 0], test_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[0].set_title('Original Uniform Distribution')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    axes[0].set_ylim(0, 1)\n",
    "    \n",
    "    axes[1].scatter(transformed_samples[:, 0], transformed_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[1].set_title('Transformed Distribution (Should be Normal)')\n",
    "    axes[1].set_xlim(-4, 4)\n",
    "    axes[1].set_ylim(-4, 4)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('u2n_flow_results.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze the normality of the transformed distribution\n",
    "    mean = np.mean(transformed_samples, axis=0)\n",
    "    std = np.std(transformed_samples, axis=0)\n",
    "    \n",
    "    print(\"Flow U2N - Mean of transformed samples:\", mean)\n",
    "    print(\"Flow U2N - Std of transformed samples:\", std)\n",
    "    \n",
    "    # Plot histograms of the transformed samples\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # x-dimension\n",
    "    sns.histplot(transformed_samples[:, 0], kde=True, stat=\"density\", ax=axes[0])\n",
    "    x = np.linspace(-4, 4, 1000)\n",
    "    axes[0].plot(x, stats.norm.pdf(x, 0, 1), 'r-', lw=2)\n",
    "    axes[0].set_title('X-dimension')\n",
    "    \n",
    "    # y-dimension\n",
    "    sns.histplot(transformed_samples[:, 1], kde=True, stat=\"density\", ax=axes[1])\n",
    "    axes[1].plot(x, stats.norm.pdf(x, 0, 1), 'r-', lw=2)\n",
    "    axes[1].set_title('Y-dimension')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('u2n_histograms.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return flow_model\n",
    "\n",
    "# Train the normal to uniform flow\n",
    "def train_normal_to_uniform_flow(n_samples=10000, epochs=100, batch_size=256):\n",
    "    # Create a dataset of normal samples\n",
    "    normal_samples = np.random.normal(0, 1, size=(n_samples, 2))\n",
    "    \n",
    "    # Create the flow model\n",
    "    flow_model = RealNVP(num_coupling_layers=8, hidden_units=128)\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)\n",
    "    \n",
    "    # Training loop\n",
    "    train_losses = []\n",
    "    \n",
    "    @tf.function\n",
    "    def train_step(z_batch):\n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = normal_to_uniform_loss(flow_model, z_batch)\n",
    "        \n",
    "        gradients = tape.gradient(loss, flow_model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, flow_model.trainable_variables))\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    # Create dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(normal_samples)\n",
    "    dataset = dataset.shuffle(buffer_size=10000).batch(batch_size)\n",
    "    \n",
    "    # Train the model\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0\n",
    "        n_batches = 0\n",
    "        \n",
    "        for z_batch in dataset:\n",
    "            loss = train_step(z_batch)\n",
    "            epoch_loss += loss\n",
    "            n_batches += 1\n",
    "        \n",
    "        epoch_loss /= n_batches\n",
    "        train_losses.append(epoch_loss.numpy())\n",
    "        \n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "    \n",
    "    # Plot the loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(train_losses)\n",
    "    plt.title('Training Loss (Normal to Uniform)')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('n2u_training_loss.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_samples = np.random.normal(0, 1, size=(5000, 2))\n",
    "    transformed_samples, _ = flow_model(test_samples)\n",
    "    transformed_samples = transformed_samples.numpy()\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].scatter(test_samples[:, 0], test_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[0].set_title('Original Normal Distribution')\n",
    "    axes[0].set_xlim(-4, 4)\n",
    "    axes[0].set_ylim(-4, 4)\n",
    "    \n",
    "    axes[1].scatter(transformed_samples[:, 0], transformed_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[1].set_title('Transformed Distribution (Should be Uniform)')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('n2u_flow_results.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Analyze the uniformity of the transformed distribution\n",
    "    mean = np.mean(transformed_samples, axis=0)\n",
    "    std = np.std(transformed_samples, axis=0)\n",
    "    \n",
    "    print(\"Flow N2U - Mean of transformed samples:\", mean)\n",
    "    print(\"Flow N2U - Std of transformed samples:\", std)\n",
    "    \n",
    "    # Test uniformity with Kolmogorov-Smirnov test\n",
    "    from scipy.stats import kstest\n",
    "    ks_result1 = kstest(transformed_samples[:, 0], 'uniform')\n",
    "    ks_result2 = kstest(transformed_samples[:, 1], 'uniform')\n",
    "    \n",
    "    print(\"KS test for dimension 1:\", ks_result1)\n",
    "    print(\"KS test for dimension 2:\", ks_result2)\n",
    "    \n",
    "    # Plot histograms of the transformed samples\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    \n",
    "    # x-dimension\n",
    "    sns.histplot(transformed_samples[:, 0], kde=True, stat=\"density\", ax=axes[0])\n",
    "    x = np.linspace(0, 1, 1000)\n",
    "    axes[0].plot(x, np.ones_like(x), 'r-', lw=2)  # Uniform density is 1\n",
    "    axes[0].set_title('X-dimension')\n",
    "    axes[0].set_xlim(0, 1)\n",
    "    \n",
    "    # y-dimension\n",
    "    sns.histplot(transformed_samples[:, 1], kde=True, stat=\"density\", ax=axes[1])\n",
    "    axes[1].plot(x, np.ones_like(x), 'r-', lw=2)  # Uniform density is 1\n",
    "    axes[1].set_title('Y-dimension')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('n2u_histograms.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return flow_model\n",
    "\n",
    "# Implement rejection sampling\n",
    "def rejection_sampling_n2u(model, n_samples, analytic=False):\n",
    "    \"\"\"\n",
    "    Use rejection sampling to generate uniform samples from normal distribution.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained normalizing flow model (or None to use analytic function)\n",
    "        n_samples: Number of samples to generate\n",
    "        analytic: Whether to use the analytic mapping or the flow model\n",
    "        \n",
    "    Returns:\n",
    "        Accepted uniform samples\n",
    "    \"\"\"\n",
    "    # We'll use the target distribution U(0,1)^2\n",
    "    # The proposal distribution is our flow model\n",
    "    \n",
    "    accepted_samples = []\n",
    "    proposal_samples = []\n",
    "    n_accepted = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    # Continue until we have enough samples\n",
    "    while n_accepted < n_samples:\n",
    "        # Get a batch of normal samples\n",
    "        batch_size = min(1000, n_samples - n_accepted)\n",
    "        z = np.random.normal(0, 1, size=(batch_size, 2))\n",
    "        \n",
    "        # Transform to uniform using the model or analytic function\n",
    "        if analytic:\n",
    "            u = normal_to_uniform_analytic(z).numpy()\n",
    "        else:\n",
    "            u, _ = model(z)\n",
    "            u = u.numpy()\n",
    "        \n",
    "        # Check if samples are in the target domain [0,1]^2\n",
    "        in_domain = np.logical_and(\n",
    "            np.logical_and(u[:, 0] >= 0, u[:, 0] <= 1),\n",
    "            np.logical_and(u[:, 1] >= 0, u[:, 1] <= 1)\n",
    "        )\n",
    "        \n",
    "        # Accept samples in the domain\n",
    "        accepted_batch = u[in_domain]\n",
    "        accepted_samples.append(accepted_batch)\n",
    "        \n",
    "        n_accepted += len(accepted_batch)\n",
    "        n_total += batch_size\n",
    "        \n",
    "        # Store some proposal samples for visualization\n",
    "        if len(proposal_samples) < 1000:\n",
    "            proposal_samples.append(u[:min(len(u), 1000 - len(proposal_samples))])\n",
    "    \n",
    "    # Combine all accepted samples\n",
    "    accepted_samples = np.vstack(accepted_samples)[:n_samples]\n",
    "    proposal_samples = np.vstack(proposal_samples)\n",
    "    \n",
    "    # Calculate acceptance rate\n",
    "    acceptance_rate = n_accepted / n_total\n",
    "    print(f\"Acceptance rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].scatter(proposal_samples[:, 0], proposal_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[0].set_title('Proposal Distribution')\n",
    "    axes[0].set_xlim(-0.5, 1.5)\n",
    "    axes[0].set_ylim(-0.5, 1.5)\n",
    "    \n",
    "    axes[1].scatter(accepted_samples[:, 0], accepted_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[1].set_title(f'Accepted Samples (Rate: {acceptance_rate:.4f})')\n",
    "    axes[1].set_xlim(0, 1)\n",
    "    axes[1].set_ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rejection_sampling_n2u.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return accepted_samples, acceptance_rate\n",
    "\n",
    "def rejection_sampling_u2n(model, n_samples, analytic=False):\n",
    "    \"\"\"\n",
    "    Use rejection sampling to generate normal samples from uniform distribution.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained normalizing flow model (or None to use analytic function)\n",
    "        n_samples: Number of samples to generate\n",
    "        analytic: Whether to use the analytic mapping or the flow model\n",
    "        \n",
    "    Returns:\n",
    "        Accepted normal samples\n",
    "    \"\"\"\n",
    "    # For normal distribution, all samples will technically be in the domain\n",
    "    # But we can reject samples that are too far from the origin\n",
    "    \n",
    "    # Threshold for accepting samples (3 std deviations)\n",
    "    threshold = 3.0\n",
    "    \n",
    "    accepted_samples = []\n",
    "    proposal_samples = []\n",
    "    n_accepted = 0\n",
    "    n_total = 0\n",
    "    \n",
    "    # Continue until we have enough samples\n",
    "    while n_accepted < n_samples:\n",
    "        # Get a batch of uniform samples\n",
    "        batch_size = min(1000, n_samples - n_accepted)\n",
    "        u = np.random.uniform(0, 1, size=(batch_size, 2))\n",
    "        \n",
    "        # Transform to normal using the model or analytic function\n",
    "        if analytic:\n",
    "            z = uniform_to_normal_analytic(u).numpy()\n",
    "        else:\n",
    "            z, _ = model(u)\n",
    "            z = z.numpy()\n",
    "        \n",
    "        # Check if samples are within a reasonable range\n",
    "        # (not strictly necessary for normal distribution but helps quality)\n",
    "        distance = np.sqrt(np.sum(z**2, axis=1))\n",
    "        in_range = distance <= threshold\n",
    "        \n",
    "        # Accept samples in range\n",
    "        accepted_batch = z[in_range]\n",
    "        accepted_samples.append(accepted_batch)\n",
    "        \n",
    "        n_accepted += len(accepted_batch)\n",
    "        n_total += batch_size\n",
    "        \n",
    "        # Store some proposal samples for visualization\n",
    "        if len(proposal_samples) < 1000:\n",
    "            proposal_samples.append(z[:min(len(z), 1000 - len(proposal_samples))])\n",
    "    \n",
    "    # Combine all accepted samples\n",
    "    accepted_samples = np.vstack(accepted_samples)[:n_samples]\n",
    "    proposal_samples = np.vstack(proposal_samples)\n",
    "    \n",
    "    # Calculate acceptance rate\n",
    "    acceptance_rate = n_accepted / n_total\n",
    "    print(f\"Acceptance rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    # Plot results\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    \n",
    "    axes[0].scatter(proposal_samples[:, 0], proposal_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[0].set_title('Proposal Distribution')\n",
    "    axes[0].set_xlim(-5, 5)\n",
    "    axes[0].set_ylim(-5, 5)\n",
    "    \n",
    "    axes[1].scatter(accepted_samples[:, 0], accepted_samples[:, 1], alpha=0.1, s=1)\n",
    "    axes[1].set_title(f'Accepted Samples (Rate: {acceptance_rate:.4f})')\n",
    "    axes[1].set_xlim(-threshold, threshold)\n",
    "    axes[1].set_ylim(-threshold, threshold)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('rejection_sampling_u2n.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return accepted_samples, acceptance_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def scalar_field_action(phi, mass=1.0, coupling=0.1, lattice_spacing=1.0):\n",
    "    \"\"\" \n",
    "    Compute the action for a scalar field configuration. \n",
    "     \n",
    "    Args: \n",
    "        phi: 2D lattice configuration of the scalar field \n",
    "        mass: Mass parameter \n",
    "        coupling: Self-coupling parameter (lambda) \n",
    "        lattice_spacing: Spacing between lattice sites \n",
    "         \n",
    "    Returns: \n",
    "        Action value \n",
    "    \"\"\"\n",
    "    nx, ny = phi.shape \n",
    "    \n",
    "    # Compute gradients using finite differences with periodic boundary conditions\n",
    "    # Gradient in x direction\n",
    "    phi_x_right = np.roll(phi, -1, axis=0)\n",
    "    phi_x_left = np.roll(phi, 1, axis=0)\n",
    "    grad_x_squared = (phi_x_right - phi)**2 / (lattice_spacing**2)\n",
    "    \n",
    "    # Gradient in y direction\n",
    "    phi_y_right = np.roll(phi, -1, axis=1)\n",
    "    phi_y_left = np.roll(phi, 1, axis=1)\n",
    "    grad_y_squared = (phi_y_right - phi)**2 / (lattice_spacing**2)\n",
    "    \n",
    "    # Kinetic term: 1/2 * [(â_x Ï)^2 + (â_y Ï)^2]\n",
    "    kinetic = 0.5 * (grad_x_squared + grad_y_squared)\n",
    "    \n",
    "    # Mass term: m^2 * Ï^2\n",
    "    mass_term = 0.5 * mass**2 * phi**2\n",
    "    \n",
    "    # Self-interaction term: Î»/4! * Ï^4\n",
    "    interaction = (coupling / 24.0) * phi**4\n",
    "    \n",
    "    # Total action: sum over all lattice sites\n",
    "    action = np.sum(kinetic + mass_term + interaction) * (lattice_spacing**2)\n",
    "    \n",
    "    return action\n",
    "\n",
    "def evaluate_scalar_field_configurations(lattice_size=10, num_configs=5, mass=1.0, coupling=0.1):\n",
    "    \"\"\"\n",
    "    Generate random scalar field configurations and evaluate their action.\n",
    "    \n",
    "    Args:\n",
    "        lattice_size: Size of the square lattice\n",
    "        num_configs: Number of configurations to evaluate\n",
    "        mass: Mass parameter\n",
    "        coupling: Self-coupling parameter\n",
    "        \n",
    "    Returns:\n",
    "        List of configurations and their corresponding actions\n",
    "    \"\"\"\n",
    "    configs = []\n",
    "    actions = []\n",
    "    suppression_factors = []\n",
    "    \n",
    "    for i in range(num_configs):\n",
    "        # Generate a random field configuration\n",
    "        phi = np.random.normal(0, 1, size=(lattice_size, lattice_size))\n",
    "        \n",
    "        # Compute the action\n",
    "        action = scalar_field_action(phi, mass, coupling)\n",
    "        \n",
    "        # Compute the suppression factor e^(-S)\n",
    "        suppression = np.exp(-action)\n",
    "        \n",
    "        configs.append(phi)\n",
    "        actions.append(action)\n",
    "        suppression_factors.append(suppression)\n",
    "        \n",
    "        print(f\"Configuration {i+1}:\")\n",
    "        print(f\"  Action S[Ï] = {action:.6f}\")\n",
    "        print(f\"  Suppression factor e^(-S) = {suppression:.6e}\")\n",
    "    \n",
    "    return configs, actions, suppression_factors\n",
    "\n",
    "def visualize_scalar_field(phi, action, suppression):\n",
    "    \"\"\"\n",
    "    Visualize a scalar field configuration and its action.\n",
    "    \n",
    "    Args:\n",
    "        phi: Scalar field configuration\n",
    "        action: Action value\n",
    "        suppression: Suppression factor e^(-S)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the scalar field configuration\n",
    "    plt.subplot(2, 1, 1)\n",
    "    im = plt.imshow(phi, cmap='viridis')\n",
    "    plt.colorbar(im, label='Field value Ï(x,y)')\n",
    "    plt.title(f'Scalar Field Configuration\\nAction S[Ï] = {action:.4f}, Suppression e^(-S) = {suppression:.4e}')\n",
    "    \n",
    "    # Plot a histogram of field values\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.hist(phi.flatten(), bins=30, alpha=0.75)\n",
    "    plt.xlabel('Field value Ï')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of field values')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "# Test the implementation with different coupling strengths\n",
    "def test_coupling_dependency(lattice_size=16, coupling_values=[0.01, 0.1, 1.0, 10.0]):\n",
    "    \"\"\"\n",
    "    Test how the suppression factor depends on the coupling strength.\n",
    "    \n",
    "    Args:\n",
    "        lattice_size: Size of the square lattice\n",
    "        coupling_values: List of coupling values to test\n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Generate a fixed random field configuration\n",
    "    phi = np.random.normal(0, 1, size=(lattice_size, lattice_size))\n",
    "    \n",
    "    results = []\n",
    "    for coupling in coupling_values:\n",
    "        action = scalar_field_action(phi, mass=1.0, coupling=coupling)\n",
    "        suppression = np.exp(-action)\n",
    "        results.append((coupling, action, suppression))\n",
    "    \n",
    "    # Print and plot results\n",
    "    print(\"Dependency on coupling strength:\")\n",
    "    for coupling, action, suppression in results:\n",
    "        print(f\"  Î» = {coupling:.2f}: S[Ï] = {action:.6f}, e^(-S) = {suppression:.6e}\")\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot([r[0] for r in results], [r[1] for r in results], 'bo-')\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('Coupling strength Î»')\n",
    "    plt.ylabel('Action S[Ï]')\n",
    "    plt.title('Action vs. Coupling Strength')\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot([r[0] for r in results], [r[2] for r in results], 'ro-')\n",
    "    plt.xscale('log')\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Coupling strength Î»')\n",
    "    plt.ylabel('Suppression factor e^(-S)')\n",
    "    plt.title('Suppression Factor vs. Coupling Strength')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return plt.gcf()\n",
    "\n",
    "def verify_action_formula():\n",
    "    \"\"\"\n",
    "    Verify that the action formula is correctly implemented by comparing\n",
    "    with analytical expectations for simple configurations.\n",
    "    \"\"\"\n",
    "    # Create a uniform zero field (should have zero action except for mass term)\n",
    "    lattice_size = 10\n",
    "    zero_field = np.zeros((lattice_size, lattice_size))\n",
    "    zero_action = scalar_field_action(zero_field, mass=1.0, coupling=0.1)\n",
    "    expected_zero_action = 0.0  # No gradients, no field values\n",
    "    print(f\"Zero field action: {zero_action:.6f}, Expected: {expected_zero_action:.6f}\")\n",
    "    \n",
    "    # Create a uniform constant field\n",
    "    const_value = 2.0\n",
    "    const_field = np.ones((lattice_size, lattice_size)) * const_value\n",
    "    const_action = scalar_field_action(const_field, mass=1.0, coupling=0.1)\n",
    "    # Expected action: mÂ²ÏÂ²/2 + Î»Ïâ´/24 per lattice site (no gradients)\n",
    "    expected_const_action = lattice_size**2 * (0.5 * 1.0**2 * const_value**2 + 0.1/24.0 * const_value**4)\n",
    "    print(f\"Constant field action: {const_action:.6f}, Expected: {expected_const_action:.6f}\")\n",
    "    \n",
    "    # Create a simple oscillating field\n",
    "    x = np.linspace(0, 2*np.pi, lattice_size)\n",
    "    y = np.linspace(0, 2*np.pi, lattice_size)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    sin_field = np.sin(X)  # Oscillates along x axis\n",
    "    sin_action = scalar_field_action(sin_field, mass=1.0, coupling=0.1)\n",
    "    print(f\"Sine field action: {sin_action:.6f}\")\n",
    "    \n",
    "    return zero_field, const_field, sin_field\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Rejection sampling for uniform to normal transformation\n",
    "def rejection_sampling_u2n(model, num_samples, M=1.5):\n",
    "    \"\"\"\n",
    "    Rejection sampling to generate samples from standard normal distribution\n",
    "    using the trained normalizing flow model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained normalizing flow model\n",
    "        num_samples: Number of samples to generate\n",
    "        M: Scaling factor for proposal distribution (should be >= 1)\n",
    "        \n",
    "    Returns:\n",
    "        Accepted samples from the target distribution\n",
    "    \"\"\"\n",
    "    accepted_samples = []\n",
    "    proposal_samples = []\n",
    "    \n",
    "    # Target density: standard normal\n",
    "    def target_density(x):\n",
    "        return np.exp(-0.5 * np.sum(x**2, axis=1))\n",
    "    \n",
    "    # Proposal density: transformed uniform with scaling factor\n",
    "    def proposal_density(u):\n",
    "        x = model(u, inverse=False).numpy()\n",
    "        # Uniform density is 1 in [0,1]Â²\n",
    "        # The density transformation includes the Jacobian determinant\n",
    "        y, ldj = model(u, inverse=False, log_det_jacobian=True)\n",
    "        return np.exp(-ldj.numpy()) / M\n",
    "    \n",
    "    total_proposed = 0\n",
    "    while len(accepted_samples) < num_samples:\n",
    "        # Generate proposal sample from uniform distribution\n",
    "        u_proposal = np.random.uniform(0, 1, size=(min(num_samples, 1000), 2)).astype(np.float32)\n",
    "        total_proposed += len(u_proposal)\n",
    "        \n",
    "        # Transform to target space\n",
    "        x_proposal = model(u_proposal, inverse=False).numpy()\n",
    "        proposal_samples.append(x_proposal)\n",
    "        \n",
    "        # Calculate acceptance ratio\n",
    "        target_vals = target_density(x_proposal)\n",
    "        proposal_vals = proposal_density(u_proposal)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        acceptance_ratio = np.zeros_like(target_vals)\n",
    "        nonzero_idx = proposal_vals > 1e-10\n",
    "        acceptance_ratio[nonzero_idx] = target_vals[nonzero_idx] / proposal_vals[nonzero_idx]\n",
    "        \n",
    "        # Accept or reject\n",
    "        u = np.random.uniform(0, 1, len(acceptance_ratio))\n",
    "        accepted_idx = u < acceptance_ratio\n",
    "        \n",
    "        if np.any(accepted_idx):\n",
    "            accepted_samples.append(x_proposal[accepted_idx])\n",
    "    \n",
    "    # Concatenate all accepted samples\n",
    "    accepted_samples = np.vstack(accepted_samples)[:num_samples]\n",
    "    proposal_samples = np.vstack(proposal_samples)\n",
    "    \n",
    "    # Calculate acceptance rate\n",
    "    acceptance_rate = len(accepted_samples) / total_proposed\n",
    "    print(f\"Acceptance rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    return accepted_samples, proposal_samples, acceptance_rate\n",
    "\n",
    "# Rejection sampling for normal to uniform transformation\n",
    "def rejection_sampling_n2u(model, num_samples, M=1.5):\n",
    "    \"\"\"\n",
    "    Rejection sampling to generate samples from uniform distribution\n",
    "    using the trained normalizing flow model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained normalizing flow model\n",
    "        num_samples: Number of samples to generate\n",
    "        M: Scaling factor for proposal distribution (should be >= 1)\n",
    "        \n",
    "    Returns:\n",
    "        Accepted samples from the target distribution\n",
    "    \"\"\"\n",
    "    accepted_samples = []\n",
    "    proposal_samples = []\n",
    "    \n",
    "    # Target density: uniform in [0,1]Â²\n",
    "    def target_density(u):\n",
    "        in_range = np.logical_and(\n",
    "            np.all(u >= 0, axis=1),\n",
    "            np.all(u <= 1, axis=1)\n",
    "        )\n",
    "        result = np.zeros(len(u))\n",
    "        result[in_range] = 1.0\n",
    "        return result\n",
    "    \n",
    "    # Proposal density: transformed normal with scaling factor\n",
    "    def proposal_density(x):\n",
    "        u = model(x, inverse=False).numpy()\n",
    "        # Normal density\n",
    "        normal_density = np.exp(-0.5 * np.sum(x**2, axis=1)) / (2 * np.pi)\n",
    "        # The density transformation includes the Jacobian determinant\n",
    "        u_transformed, ldj = model(x, inverse=False, log_det_jacobian=True)\n",
    "        return normal_density * np.exp(ldj.numpy()) / M\n",
    "    \n",
    "    total_proposed = 0\n",
    "    while len(accepted_samples) < num_samples:\n",
    "        # Generate proposal sample from normal distribution\n",
    "        x_proposal = np.random.normal(0, 1, size=(min(num_samples, 1000), 2)).astype(np.float32)\n",
    "        total_proposed += len(x_proposal)\n",
    "        \n",
    "        # Transform to target space\n",
    "        u_proposal = model(x_proposal, inverse=False).numpy()\n",
    "        proposal_samples.append(u_proposal)\n",
    "        \n",
    "        # Calculate acceptance ratio\n",
    "        target_vals = target_density(u_proposal)\n",
    "        proposal_vals = proposal_density(x_proposal)\n",
    "        \n",
    "        # Avoid division by zero\n",
    "        acceptance_ratio = np.zeros_like(target_vals)\n",
    "        nonzero_idx = proposal_vals > 1e-10\n",
    "        acceptance_ratio[nonzero_idx] = target_vals[nonzero_idx] / proposal_vals[nonzero_idx]\n",
    "        \n",
    "        # Accept or reject\n",
    "        u = np.random.uniform(0, 1, len(acceptance_ratio))\n",
    "        accepted_idx = u < acceptance_ratio\n",
    "        \n",
    "        if np.any(accepted_idx):\n",
    "            accepted_samples.append(u_proposal[accepted_idx])\n",
    "    \n",
    "    # Concatenate all accepted samples\n",
    "    accepted_samples = np.vstack(accepted_samples)[:num_samples]\n",
    "    proposal_samples = np.vstack(proposal_samples)\n",
    "    \n",
    "    # Calculate acceptance rate\n",
    "    acceptance_rate = len(accepted_samples) / total_proposed\n",
    "    print(f\"Acceptance rate: {acceptance_rate:.4f}\")\n",
    "    \n",
    "    return accepted_samples, proposal_samples, acceptance_rate\n",
    "\n",
    "# Test rejection sampling for both models\n",
    "def test_rejection_sampling(model, untrained_model=None):\n",
    "    print(\"Testing rejection sampling for uniform to normal transformation\")\n",
    "    \n",
    "    if untrained_model is None:\n",
    "        # Create an untrained model for comparison\n",
    "        untrained_model = RealNVP(num_coupling_layers=4)\n",
    "    \n",
    "    # Test trained model\n",
    "    print(\"Trained model:\")\n",
    "    accepted_samples_trained, proposal_samples_trained, rate_trained = rejection_sampling_u2n(model, 1000)\n",
    "    \n",
    "    # Test untrained model\n",
    "    print(\"Untrained model:\")\n",
    "    accepted_samples_untrained, proposal_samples_untrained, rate_untrained = rejection_sampling_u2n(untrained_model, 1000)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot samples from trained model\n",
    "    axs[0, 0].scatter(proposal_samples_trained[:500, 0], proposal_samples_trained[:500, 1], \n",
    "                      alpha=0.3, s=5, c='blue', label='Proposed')\n",
    "    axs[0, 0].scatter(accepted_samples_trained[:500, 0], accepted_samples_trained[:500, 1], \n",
    "                      alpha=0.5, s=5, c='red', label='Accepted')\n",
    "    axs[0, 0].set_title(f'Trained Model (Acceptance Rate: {rate_trained:.4f})')\n",
    "    axs[0, 0].legend()\n",
    "    axs[0, 0].set_xlim(-4, 4)\n",
    "    axs[0, 0].set_ylim(-4, 4)\n",
    "    \n",
    "    # Plot samples from untrained model\n",
    "    axs[0, 1].scatter(proposal_samples_untrained[:500, 0], proposal_samples_untrained[:500, 1], \n",
    "                      alpha=0.3, s=5, c='blue', label='Proposed')\n",
    "    axs[0, 1].scatter(accepted_samples_untrained[:500, 0], accepted_samples_untrained[:500, 1], \n",
    "                      alpha=0.5, s=5, c='red', label='Accepted')\n",
    "    axs[0, 1].set_title(f'Untrained Model (Acceptance Rate: {rate_untrained:.4f})')\n",
    "    axs[0, 1].legend()\n",
    "    axs[0, 1].set_xlim(-4, 4)\n",
    "    axs[0, 1].set_ylim(-4, 4)\n",
    "    \n",
    "    # Plot histograms of accepted samples\n",
    "    axs[1, 0].hist(accepted_samples_trained[:, 0], bins=30, alpha=0.5, density=True, label='Dimension 1')\n",
    "    axs[1, 0].hist(accepted_samples_trained[:, 1], bins=30, alpha=0.5, density=True, label='Dimension 2')\n",
    "    x_range = np.linspace(-4, 4, 1000)\n",
    "    normal_pdf = stats.norm.pdf(x_range)\n",
    "    axs[1, 0].plot(x_range, normal_pdf, 'r-', lw=2, label='Standard Normal PDF')\n",
    "    axs[1, 0].set_title('Trained Model: Marginal Distributions')\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    axs[1, 1].hist(accepted_samples_untrained[:, 0], bins=30, alpha=0.5, density=True, label='Dimension 1')\n",
    "    axs[1, 1].hist(accepted_samples_untrained[:, 1], bins=30, alpha=0.5, density=True, label='Dimension 2')\n",
    "    axs[1, 1].plot(x_range, normal_pdf, 'r-', lw=2, label='Standard Normal PDF')\n",
    "    axs[1, 1].set_title('Untrained Model: Marginal Distributions')\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(f\"Acceptance rate comparison:\")\n",
    "    print(f\"  Trained model: {rate_trained:.4f}\")\n",
    "    print(f\"  Untrained model: {rate_untrained:.4f}\")\n",
    "    print(f\"  Improvement factor: {rate_trained / rate_untrained:.2f}x\")\n",
    "    \n",
    "    return fig, (rate_trained, rate_untrained)\n",
    "\n",
    "# Similar function for normal to uniform transformation\n",
    "def test_rejection_sampling_n2u(model, untrained_model=None):\n",
    "    print(\"Testing rejection sampling for normal to uniform transformation\")\n",
    "    \n",
    "    if untrained_model is None:\n",
    "        # Create an untrained model for comparison\n",
    "        untrained_model = RealNVP(num_coupling_layers=4)\n",
    "    \n",
    "    # Test trained model\n",
    "    print(\"Trained model:\")\n",
    "    accepted_samples_trained, proposal_samples_trained, rate_trained = rejection_sampling_n2u(model, 1000)\n",
    "    \n",
    "    # Test untrained model\n",
    "    print(\"Untrained model:\")\n",
    "    accepted_samples_untrained, proposal_samples_untrained, rate_untrained = rejection_sampling_n2u(untrained_model, 1000)\n",
    "    \n",
    "    # Visualize results\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
    "    \n",
    "    # Plot samples from trained model\n",
    "    axs[0, 0].scatter(accepted_samples_trained[:500, 0], accepted_samples_trained[:500, 1], \n",
    "                      alpha=0.5, s=5, c='red')\n",
    "    axs[0, 0].set_title(f'Trained Model (Acceptance Rate: {rate_trained:.4f})')\n",
    "    axs[0, 0].set_xlim(0, 1)\n",
    "    axs[0, 0].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot samples from untrained model\n",
    "    axs[0, 1].scatter(accepted_samples_untrained[:500, 0], accepted_samples_untrained[:500, 1], \n",
    "                      alpha=0.5, s=5, c='red')\n",
    "    axs[0, 1].set_title(f'Untrained Model (Acceptance Rate: {rate_untrained:.4f})')\n",
    "    axs[0, 1].set_xlim(0, 1)\n",
    "    axs[0, 1].set_ylim(0, 1)\n",
    "    \n",
    "    # Plot histograms of accepted samples\n",
    "    axs[1, 0].hist(accepted_samples_trained[:, 0], bins=30, alpha=0.5, density=True, label='Dimension 1')\n",
    "    axs[1, 0].hist(accepted_samples_trained[:, 1], bins=30, alpha=0.5, density=True, label='Dimension 2')\n",
    "    axs[1, 0].plot([0, 0, 1, 1], [0, 1, 1, 0], 'r-', lw=2, label='Uniform PDF')\n",
    "    axs[1, 0].set_title('Trained Model: Marginal Distributions')\n",
    "    axs[1, 0].set_ylim(0, 1.5)\n",
    "    axs[1, 0].legend()\n",
    "    \n",
    "    axs[1, 1].hist(accepted_samples_untrained[:, 0], bins=30, alpha=0.5, density=True, label='Dimension 1')\n",
    "    axs[1, 1].hist(accepted_samples_untrained[:, 1], bins=30, alpha=0.5, density=True, label='Dimension 2')\n",
    "    axs[1, 1].plot([0, 0, 1, 1], [0, 1, 1, 0], 'r-', lw=2, label='Uniform PDF')\n",
    "    axs[1, 1].set_title('Untrained Model: Marginal Distributions')\n",
    "    axs[1, 1].set_ylim(0, 1.5)\n",
    "    axs[1, 1].legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    print(f\"Acceptance rate comparison:\")\n",
    "    print(f\"  Trained model: {rate_trained:.4f}\")\n",
    "    print(f\"  Untrained model: {rate_untrained:.4f}\")\n",
    "    print(f\"  Improvement factor: {rate_trained / rate_untrained:.2f}x\")\n",
    "    \n",
    "    return fig, (rate_trained, rate_untrained)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Create and train models (this would be from the previous code)\n",
    "    # model = RealNVP(num_coupling_layers=4)  # Trained model from part 2\n",
    "    # model_n2u = RealNVP(num_coupling_layers=4)  # Trained model from part 3\n",
    "    \n",
    "    # Create models for demonstration (these would be loaded from trained models)\n",
    "    model = RealNVP(num_coupling_layers=4)\n",
    "    model_n2u = RealNVP(num_coupling_layers=4)\n",
    "    \n",
    "    # Test rejection sampling\n",
    "    fig_u2n, rates_u2n = test_rejection_sampling(model)\n",
    "    fig_n2u, rates_n2u = test_rejection_sampling_n2u(model_n2u)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
